# Optimizing the Optimizers (NIPS 2016)

**Barcelona, Spain, December 9 or 10, 2016**

<td width="100%" style="position: relative;">
    <img src="../../_static/img/meetings/1280px-Port_Vell_Barcelona_2007.jpg" width="100%">
    <div>
        <small>
            Barcelona by
            <a href="http://en.wikipedia.org/wiki/User:Diliff">
            David Iliff
            </a> /
            <a href="https://creativecommons.org/licenses/by-sa/3.0/deed.en">
                CC-BY-SA_3.5
            </a>
        </small>
    </div>
</td>

Optimization problems in machine learning have aspects that make them more challenging than the traditional settings, like stochasticity, and parameters with side-effects (e.g., the batch size and structure). The field has invented many different approaches to deal with these demands. Unfortunately - and intriguingly - this extra functionality seems to invariably necessitate the introduction of tuning parameters: step sizes, decay rates, cycle lengths, batch sampling distributions, and so on. Such parameters are not present, or at least not as prominent, in classic optimization methods. But getting them right is frequently crucial, and necessitates inconvenient human “babysitting”.

Recent work has increasingly tried to eliminate such fiddle factors, typically by statistical estimation. This also includes automatic selection of external parameters like the batch-size or -structure, which have not traditionally been treated as part of the optimization task. Several different strategies have now been proposed, but they are not always compatible with each other, and lack a common framework that would foster both conceptual and algorithmic interoperability. This workshop aims to provide a forum for the nascent community studying automating parameter-tuning in optimization routines.

Among the questions to be addressed by the workshop are:

* Is the prominence of tuning parameters a fundamental feature of stochastic optimization problems? Why do classic optimization methods manage to do well with virtually no free parameters?
* In which precise sense can the "optimization of optimization algorithms" be phrased as an inference / learning problem?
* Should, and can, parameters be inferred at design-time (by a human), at compile-time (by an external compiler with access to a meta-description of the problem) or run-time (by the algorithm itself)?
* What are generic ways to learn parameters of algorithms, and inherent difficulties for doing so? Is the goal to *specialize* to a particular problem, or to *generalize* over many problems?

The workshop is organized by [Maren Mahsereci](https://ei.is.tuebingen.mpg.de/person/mmahsereci), [Alex Davies](http://www.alexdavies.net) and [Philipp Hennig.](http://ei.is.tuebingen.mpg.de/person/phennig)

<!--
### Confirmed Speakers
* [Stephen J Wright (U of Wisconsin)](http://pages.cs.wisc.edu/~swright/)
* [Mark Schmidt (UBC)](http://www.cs.ubc.ca/~schmidtm/)
* [David Duvenaud (Harvard -> Toronto)](http://www.cs.toronto.edu/~duvenaud/)
* [Matthew Hoffman (DeepMind)](http://mlg.eng.cam.ac.uk/hoffmanm/)
* [Samantha Hansen (Spotify)](https://www.linkedin.com/in/samantha-hansen-a37585105)
* [Ali Rahimi (Google)](https://keysduplicated.com/~ali/)
* [Ameet Talwalkar (UCLA)](http://web.cs.ucla.edu/~ameet/)
-->

## Schedule

The workshop will be held on Saturday, 10 December, in *Area 2*

| Time | | Event | Material |
| :--------------- | - | :---- | :---- |
| **09:00-09:10** | --- |  *Opening Remarks* | |
| **09:10-09:30** | --- |  [Matt Hoffman (DeepMind)](http://mlg.eng.cam.ac.uk/hoffmanm/) | |
| **09:30-10:00** | --- |  [David Duvenaud (U Toronto)](http://www.cs.toronto.edu/~duvenaud/) | <a href="../../_static/pdf/meetings/NIPS2016/talks/DavidDuvenaud.pdf">(slides)</a> |
| **10:00-10:30** | --- |  [Stephen J Wright (U of Wisconsin)](http://pages.cs.wisc.edu/~swright/) | <a href="../../_static/pdf/meetings/NIPS2016/talks/StephenWright.pdf">(slides)</a> |
| **10.30-11.00** | --- |  *Coffee Break* | |
| **11:00-11:30** | --- |  [Samantha Hansen (Spotify)](https://www.linkedin.com/in/samantha-hansen-a37585105) | <a href="../../_static/pdf/meetings/NIPS2016/talks/SammyHansen.pptx">(slides)<a> |
| **11:30-12:00** | --- |  *Spotlights* | (see below) |
| **12:00-12:45** | --- |  *Poster Session* | |
| **12:45-14:15** | --- |  *Lunch Break* | |
| **14:15-14:40** | --- |  [Matteo Pirotta (Politecnico di Milano)](https://teopir.github.io) | |
| **14:40-15:00** | --- |  [Ameet Talwalkar (UCLA)](http://web.cs.ucla.edu/~ameet/)  | <a href="../../_static/pdf/meetings/NIPS2016/talks/AmeetTalwalkar.pdf">(slides)</a> |
| **15:00-15:30** | --- |  *Coffee Break* | |
| **15:30-15:50** | --- |  [Ali Rahimi (Google)](https://keysduplicated.com/~ali/) | |
| **15:50-16.20** | --- |  [Mark Schmidt (UBC)](http://www.cs.ubc.ca/~schmidtm/) | |
| **16:20-17:00** | --- |  *Panel Discussion* | |


## Accepted Papers
(in alphabetical order, by first author's surname)

* Ömer Deniz Akyildiz, Víctor Elvira, Jesus Fernandez-Bes, Joaquín Miguez. <a href="../../_static/pdf/meetings/NIPS2016/papers/Akyildiz_Elvira_Fernandez-Bes_Miguez.pdf">*On the Relationship between Online Optimizers and Recursive Filters*</a>]
* Matt Bonakdarpour and Panagiotis (Panos) Toulis. <a href="../../_static/pdf/meetings/NIPS2016/papers/Bonakdarpour_Toulis.pdf">*Statistical Perspectives of Stochastic Optimization*</a>
* Anirban Chaudhuri, David Wolpert, Brendan Tracey. <a href="../../_static/pdf/meetings/NIPS2016/papers/Chaudhuri_Wolpert_Tracey.pdf">*Stochastic Optimization and Machine Learning: Cross-Validation for Cross-Entropy Method*</a>
* Kamil Ciosek and Shimon Whiteson. <a href="../../_static/pdf/meetings/NIPS2016/papers/Ciosek_Whiteson.pdf">*Off-Environment RL with Rare Events*</a>
* Guilherme França and José Bento. <a href="../../_static/pdf/meetings/NIPS2016/papers/Franca_Bento.pdf">*Tuning Over-Relaxed ADMM*</a>
* Tobias Glasmachers. <a href="../../_static/pdf/meetings/NIPS2016/papers/Glasmachers.pdf">*Small Stochastic Average Gradient Steps*</a>
* Ke Li and Jitendra Malik. <a href="../../_static/pdf/meetings/NIPS2016/papers/Li_Malik.pdf">*Learning to Optimize*</a>
* Ben London. <a href="../../_static/pdf/meetings/NIPS2016/papers/London.pdf">*Generalization Bounds for Randomized Learning with Application to Stochastic Gradient Descent*</a>
* Matteo Pirotta and Marcello Restelli. <a href="../../_static/pdf/meetings/NIPS2016/papers/Pirotta_Restelli.pdf">*Cost-Sensitive Approach for Batch Size Optimization*</a>

<!-- ## Call for Papers

### Important dates

* Submission deadline: 18:00 GMT, 7 October 2016 (deadline extended)
* Notification of acceptance: 7 November 2016

### Topics of interest

* Parameter adaptation for optimization algorithms
* Stochastic optimization methods
* Optimization methods adapted for specific applications
* Batch selection methods
* Convergence diagnostics for optimization algorithms

We welcome contributions from theoretical treatments, empirical studies, and applications, of the above. The list is not exhaustive, and we also welcome submissions that draw upon highly related topics.

### Submission instructions

Submissions should be in the (new!) NIPS 2016 format, with a maximum of 4 pages (excluding references). Accepted papers will be made available online at the workshop website, and will be presented in a spotlight talk at the workshop itself, but the workshop proceedings can be considered non-archival. Explicitly: shorter versions of relevant papers submitted or published elsewhere are encouraged. Submissions need not be anonymous.

Please mail pdf submissions to <mmahsereci@tue.mpg.de>. -->
