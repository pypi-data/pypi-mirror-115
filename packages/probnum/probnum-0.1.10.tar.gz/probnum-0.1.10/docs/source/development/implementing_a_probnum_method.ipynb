{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Probabilistic Numerical Method\n",
    "\n",
    "In this guide we will explain how to implement a probabilistic numerical method in ProbNum. As a simple example problem we consider optimization of a possibly noisy quadratic objective $f : \\mathbb{R} \\rightarrow \\mathbb{R}$. We assume we only have access to function evaluations\n",
    "$$\n",
    "y = f(x) = \\frac{1}{2}ax^2 + bx + c\n",
    "$$\n",
    "\n",
    "where in the stochastic case the parameters are corrupted by an additive noise term.\n",
    "\n",
    "This particular problem was used to showcase *what* a PN method *is* in the [tutorial on PN methods](../tutorials/pn_methods.ipynb). Now we will turn to *how* to implement such a method in ProbNum. This tutorial is split into two parts. The first outlines the interface, the function most users will call. The second explains the class which actually implements the algorithm.\n",
    "\n",
    "Note, this development tutorial focusses on explaining the design and structure of the implementation of a probabilistic numerical method, not necessarily on efficiency for the solution of this particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface\n",
    "\n",
    "Each probabilistic numerical method in ProbNum has an interface defined via a function, e.g. [probnum.diffeq.probsolve_ivp](../api/automod/probnum.diffeq.probsolve_ivp.rst#probnum.diffeq.probsolve_ivp). This is the primary way the average user of ProbNum interacts with the library. Such an interface will typically allow different variants of the same probabilistic numerical method to be called depending on the problem or a user-supplied argument. Similar designs can be found in other libraries (see e.g. [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize)).\n",
    "\n",
    "### Signature\n",
    "\n",
    "If your method has a classic analogue in NumPy or SciPy, make sure the method signatures match as closely as possible. This enables PN methods to be used as drop-in replacements for classic numerical routines, e.g.\n",
    "\n",
    "```python\n",
    "# Solve using NumPy\n",
    "x = np.linalg.solve(A, b)\n",
    "\n",
    "# Solve using ProbNum\n",
    "x_rv, _, _, info = pn.linalg.problinsolve(A, b)\n",
    "```\n",
    "\n",
    "In the case where a classic numerical method exists, the naming scheme typically adheres to the convention `bayes<classic_method>()` or `prob<classic_method>()`. In our example we want our function to be callable as\n",
    "\n",
    "```python\n",
    "x, _, _, info = pn.optim.probsolve_qp(lambda x: 2 * x ** 2 - 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method `probsolve_qp`\n",
    "\n",
    "We will now take a closer look at the interface of our 1D noisy quadratic optimization method. At a basic level `probsolve_qp` takes a function of the type `Callable[[FloatArgType], FloatArgType]`. This hints that the optimization objective is a 1D function. Our prior knowledge about the parameters $(a,b,c)$ is encoded in the random variable `fun_params0`. However, we want to also give a user the option to not specify any prior knowledge or just a guess about the parameter values, hence this argument is optional or can be an `np.ndarray`. \n",
    "\n",
    "The interface also has an `assume_fun` argument, which allows specification of the variant of the probabilistic numerical method to use based on the assumptions about the problem. For convenience, this can be inferred from the problem itself. The actual implementation of the PN method variant which is initialized in a modular fashion is separate from the interface and will be explained later. Finally, the actual optimization routine is called and the result is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union, Optional, Tuple, Dict, Iterable\n",
    "import numpy as np\n",
    "\n",
    "import probnum as pn\n",
    "from probnum import randvars, linops\n",
    "from probnum.typing import FloatArgType, IntArgType\n",
    "\n",
    "rng = np.random.default_rng(seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load -s probsolve_qp quadopt_example/_probsolve_qp\n",
    "def probsolve_qp(\n",
    "    rng: np.random.Generator,\n",
    "    fun: Callable[[FloatArgType], FloatArgType],\n",
    "    fun_params0: Optional[Union[np.ndarray, randvars.RandomVariable]] = None,\n",
    "    assume_fun: Optional[str] = None,\n",
    "    tol: FloatArgType = 10 ** -5,\n",
    "    maxiter: IntArgType = 10 ** 4,\n",
    "    noise_cov: Optional[Union[np.ndarray, linops.LinearOperator]] = None,\n",
    "    callback: Optional[\n",
    "        Callable[[FloatArgType, FloatArgType, randvars.RandomVariable], None]\n",
    "    ] = None,\n",
    ") -> Tuple[float, randvars.RandomVariable, randvars.RandomVariable, Dict]:\n",
    "    \"\"\"Probabilistic 1D Quadratic Optimization.\n",
    "\n",
    "    PN method solving unconstrained one-dimensional (noisy) quadratic\n",
    "    optimization problems only needing access to function evaluations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rng :\n",
    "        Random number generator.\n",
    "    fun :\n",
    "        Quadratic objective function to optimize.\n",
    "    fun_params0 :\n",
    "        *(shape=(3, ) or (3, 1))* -- Prior on the parameters of the\n",
    "        objective function or initial guess for the parameters.\n",
    "    assume_fun :\n",
    "        Type of probabilistic numerical method to use. The available\n",
    "        options are\n",
    "\n",
    "        =====================  =============\n",
    "         automatic selection   ``None``\n",
    "         exact observations    ``\"exact\"``\n",
    "         noisy observations    ``\"noise\"``\n",
    "        =====================  =============\n",
    "\n",
    "        If ``None`` the type of method is inferred from the problem\n",
    "        ``fun`` and prior ``fun_params0``.\n",
    "    tol :\n",
    "        Convergence tolerance.\n",
    "    maxiter :\n",
    "        Maximum number of iterations.\n",
    "    noise_cov :\n",
    "        *(shape=(3, 3))* -- Covariance of the additive noise on the parameters\n",
    "        of the noisy objective function.\n",
    "    callback :\n",
    "        Callback function returning intermediate quantities of the\n",
    "        optimization loop. Note that depending on the function\n",
    "        supplied, this can slow down the solver considerably.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_opt :\n",
    "        Estimated minimum of the objective function.\n",
    "    fun_opt :\n",
    "        Belief over the optimal value of the objective function.\n",
    "    fun_params :\n",
    "        Belief over the parameters of the objective function.\n",
    "    info :\n",
    "        Additional information about the optimization, e.g. convergence.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> f = lambda x: 2.0 * x ** 2 - 0.75 * x + 0.2\n",
    "    >>> x_opt, fun_opt, fun_params_opt, info = probsolve_qp(f)\n",
    "    >>> print(info[\"iter\"])\n",
    "    3\n",
    "    \"\"\"\n",
    "\n",
    "    # Choose a variant of the PN method\n",
    "    if assume_fun is None:\n",
    "        # Infer PN variant to use based on the problem\n",
    "        if noise_cov is not None or fun(1.0) != fun(1.0):\n",
    "            assume_fun = \"noise\"\n",
    "        else:\n",
    "            assume_fun = \"exact\"\n",
    "\n",
    "    # Select appropriate prior based on the problem\n",
    "    fun_params0 = _choose_prior(fun_params0=fun_params0)\n",
    "\n",
    "    if assume_fun == \"exact\":\n",
    "        # Exact 1D quadratic optimization\n",
    "        probquadopt = ProbabilisticQuadraticOptimizer(\n",
    "            fun_params_prior=fun_params0,\n",
    "            policy=partial(stochastic_policy, rng=rng),\n",
    "            observation_operator=function_evaluation,\n",
    "            belief_update=partial(gaussian_belief_update, noise_cov=np.zeros(3)),\n",
    "            stopping_criteria=[\n",
    "                partial(parameter_uncertainty, abstol=tol, reltol=tol),\n",
    "                partial(maximum_iterations, maxiter=maxiter),\n",
    "            ],\n",
    "        )\n",
    "    elif assume_fun == \"noise\":\n",
    "        # Noisy 1D quadratic optimization\n",
    "        probquadopt = ProbabilisticQuadraticOptimizer(\n",
    "            fun_params_prior=fun_params0,\n",
    "            policy=partial(explore_exploit_policy, rng=rng),\n",
    "            observation_operator=function_evaluation,\n",
    "            belief_update=partial(gaussian_belief_update, noise_cov=noise_cov),\n",
    "            stopping_criteria=[\n",
    "                partial(parameter_uncertainty, abstol=tol, reltol=tol),\n",
    "                partial(maximum_iterations, maxiter=maxiter),\n",
    "            ],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Unknown assumption on function evaluations: \"{assume_fun}\".')\n",
    "\n",
    "    # Run optimization iteration\n",
    "    x_opt0, fun_opt0, fun_params0, info = probquadopt.optimize(\n",
    "        fun=fun, callback=callback\n",
    "    )\n",
    "\n",
    "    # Return output with information (e.g. on convergence)\n",
    "    info[\"assume_fun\"] = assume_fun\n",
    "    return x_opt0, fun_opt0, fun_params0, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call `probsolve_qp` to solve a simple deterministic quadratic optimization problem. We only specify the problem and let the interface determine the problem type, prior information and appropriate variant of the solver. As expected, we converge in three iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iter': 3, 'conv_crit': 'uncertainty_abstol', 'assume_fun': 'exact'}\n",
      "[ 4.  -0.8  0.2]\n",
      "0.20000000000000004\n"
     ]
    }
   ],
   "source": [
    "import probnum\n",
    "from quadopt_example import probsolve_qp\n",
    "\n",
    "# Objective function\n",
    "fun = lambda x: 2.0 * x ** 2 - 0.8 * x + 0.2\n",
    "\n",
    "# Probabilistic 1D quadratic optimization\n",
    "x_opt, fun_opt, fun_params, info = probsolve_qp(rng=rng, fun=fun)\n",
    "\n",
    "print(info)\n",
    "print(fun_params.mean)\n",
    "print(x_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider a noise-corrupted problem. We will provide a guess for the parameters of the quadratic. The interface automatically detects that it is faced with a noisy problem and an appropriate prior aligned with the provided point estimate for the parameters `fun_params0` is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iter': 8, 'conv_crit': 'uncertainty_reltol', 'assume_fun': 'noise'}\n",
      "[ 4.01060655 -0.8054893   0.202631  ]\n",
      "0.20083977160526964\n"
     ]
    }
   ],
   "source": [
    "# Stochastic quadratic objective\n",
    "noise_cov = 0.00005 * np.eye(3)\n",
    "\n",
    "\n",
    "def fun_hat(x):\n",
    "    a, b, c = randvars.Normal(np.array([4.0, -0.8, 0.2]), noise_cov).sample(rng=rng)\n",
    "    return 0.5 * a * x ** 2 + b * x + c\n",
    "\n",
    "\n",
    "x_opt, fun_opt, fun_params, info = probsolve_qp(\n",
    "    rng, fun_hat, fun_params0=np.array([1.0, -1.0, 0.0]), noise_cov=noise_cov\n",
    ")\n",
    "\n",
    "print(info)\n",
    "print(fun_params.mean)\n",
    "print(x_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you add your own new interface, look through the codebase whether an interface for your PN method exists. You can then simply write an implementation (see below) and call it from the interface function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "There are often different variations of a given (probabilistic) numerical routine depending on the arguments supplied. In ProbNum these are implemented in a compositional manner. This allows more advanced users to create custom variants and more easily allows additions and changes to the interface. Implementing the solver in a compositional manner means that its components, i.e. policy, observation process, belief update and stopping criteria can be easily \"swapped out\" with minor changes to the code. Typically, [compositionality is preferred over inheritance](https://en.wikipedia.org/wiki/Composition_over_inheritance) since it avoids having to create combinatorially many new classes when adding a new method variant.\n",
    "\n",
    "The components of the `ProbabilisticQuadraticOptimizer` are implemented as functions in separate modules. The different variants of the same component all share the same initial arguments and may have additional ones adjusting their behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class `ProbabilisticQuadraticOptimizer`\n",
    "\n",
    "We will start by getting an overview of the actual class implementing the 1D probabilistic quadratic optimizer. Based on the compositional pattern it pulls together all the components and calls them with their respective arguments in the right order. \n",
    "\n",
    "The optimization iteration `.optimize` steps through a generator given by `.optim_iterator` one action and observation at a time until any of the stopping criteria are fulfilled. The generator gives control to an advanced user over the output of each iteration and allows more granular testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 17- quadopt_example/probabilistic_quadratic_optimizer.py\n",
    "# Type aliases for quadratic optimization\n",
    "QuadOptPolicyType = Callable[\n",
    "    [\n",
    "        Callable[[FloatArgType], FloatArgType],\n",
    "        randvars.RandomVariable,\n",
    "    ],\n",
    "    FloatArgType,\n",
    "]\n",
    "QuadOptObservationOperatorType = Callable[\n",
    "    [Callable[[FloatArgType], FloatArgType], FloatArgType], FloatArgType\n",
    "]\n",
    "QuadOptBeliefUpdateType = Callable[\n",
    "    [\n",
    "        randvars.RandomVariable,\n",
    "        FloatArgType,\n",
    "        FloatArgType,\n",
    "    ],\n",
    "    randvars.RandomVariable,\n",
    "]\n",
    "QuadOptStoppingCriterionType = Callable[\n",
    "    [Callable[[FloatArgType], FloatArgType], randvars.RandomVariable, IntArgType],\n",
    "    Tuple[bool, Union[str, None]],\n",
    "]\n",
    "\n",
    "\n",
    "class ProbabilisticQuadraticOptimizer:\n",
    "    \"\"\"Probabilistic Quadratic Optimization in 1D.\n",
    "\n",
    "    PN method solving unconstrained one-dimensional (noisy) quadratic\n",
    "    optimization problems only needing access to function evaluations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fun_params_prior :\n",
    "        Prior belief over the parameters of the latent quadratic function.\n",
    "    policy :\n",
    "        Callable returning a new action to probe the problem.\n",
    "    observation_operator :\n",
    "        Callable implementing the observation process of the problem.\n",
    "    belief_update :\n",
    "        Belief update function updating the belief over the parameters of the quadratic given\n",
    "        an action and observation of the problem.\n",
    "    stopping_criteria :\n",
    "        Stopping criteria to determine when to stop the optimization.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    probsolve_qp : Solve 1D (noisy) quadratic optimization problems.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from functools import partial\n",
    "    >>> import numpy as np\n",
    "    >>> from probnum import randvars\n",
    "    >>> from quadopt_example.policies import stochastic_policy\n",
    "    >>> from quadopt_example.observation_operators import function_evaluation\n",
    "    >>> from quadopt_example.belief_updates import gaussian_belief_update\n",
    "    >>> from quadopt_example.stopping_criteria import maximum_iterations\n",
    "    >>>\n",
    "    >>>\n",
    "    >>> # Custom stopping criterion based on residual\n",
    "    >>> def residual(\n",
    "    >>>     fun,\n",
    "    >>>     fun_params0,\n",
    "    >>>     current_iter,\n",
    "    >>>     abstol=10 ** -6,\n",
    "    >>>     reltol=10 ** -6,\n",
    "    >>> ):\n",
    "    >>>     a, b, c = fun_params0.mean\n",
    "    >>>     resid = np.abs(fun(1.0) - (0.5 * a + b + c))\n",
    "    >>>     if resid < abstol:\n",
    "    >>>         return True, \"residual_abstol\"\n",
    "    >>>     elif resid < np.abs(fun(1.0)) * reltol:\n",
    "    >>>         return True, \"residual_reltol\"\n",
    "    >>>     else:\n",
    "    >>>         return False, None\n",
    "    >>>\n",
    "    >>> # Compose custom PN method\n",
    "    >>> quadopt = ProbabilisticQuadraticOptimizer(\n",
    "    >>>     fun_params_prior=randvars.Normal(np.zeros(3), np.eye(3)),\n",
    "    >>>     policy=stochastic_policy,\n",
    "    >>>     observation_operator=function_evaluation,\n",
    "    >>>     belief_update=partial(gaussian_belief_update, noise_cov=np.zeros(3)),\n",
    "    >>>     stopping_criteria=[residual, partial(maximum_iterations, maxiter=10)],\n",
    "    >>> )\n",
    "    >>> # Objective function\n",
    "    >>> f = lambda x: 2.0 * x ** 2 - 0.75 * x + 0.2\n",
    "    >>>\n",
    "    >>> quadopt.optimize(f)\n",
    "    (0.2000000000000014,\n",
    "     <() Normal with dtype=float64>,\n",
    "     <(3,) Normal with dtype=float64>,\n",
    "     {'iter': 3, 'conv_crit': 'residual_abstol'})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fun_params_prior: randvars.RandomVariable,\n",
    "        policy: QuadOptPolicyType,\n",
    "        observation_operator: QuadOptObservationOperatorType,\n",
    "        belief_update: QuadOptBeliefUpdateType,\n",
    "        stopping_criteria: Union[\n",
    "            QuadOptStoppingCriterionType, Iterable[QuadOptStoppingCriterionType]\n",
    "        ],\n",
    "    ):\n",
    "        # Optimizer components\n",
    "        self.fun_params = fun_params_prior\n",
    "        self.policy = policy\n",
    "        self.observation_operator = observation_operator\n",
    "        self.belief_update = belief_update\n",
    "\n",
    "        if not isinstance(stopping_criteria, collections.abc.Iterable):\n",
    "            self.stopping_criteria = [stopping_criteria]\n",
    "        else:\n",
    "            self.stopping_criteria = stopping_criteria\n",
    "\n",
    "    def has_converged(\n",
    "        self, fun: Callable[[FloatArgType], FloatArgType], iteration: IntArgType\n",
    "    ) -> Tuple[bool, Union[str, None]]:\n",
    "        \"\"\"Check whether the optimizer has converged.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fun :\n",
    "            Quadratic objective function to optimize.\n",
    "        iteration :\n",
    "            Number of iterations of the solver performed up to this point.\n",
    "        \"\"\"\n",
    "        for stopping_criterion in self.stopping_criteria:\n",
    "            _has_converged, convergence_criterion = stopping_criterion(\n",
    "                fun, self.fun_params, iteration\n",
    "            )\n",
    "            if _has_converged:\n",
    "                return True, convergence_criterion\n",
    "        return False, None\n",
    "\n",
    "    def optim_iterator(\n",
    "        self,\n",
    "        fun: Callable[[FloatArgType], FloatArgType],\n",
    "    ) -> Tuple[float, float, randvars.RandomVariable]:\n",
    "        \"\"\"Generator implementing the optimization iteration.\n",
    "\n",
    "        This function allows stepping through the optimization\n",
    "        process one step at a time.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fun :\n",
    "            Quadratic objective function to optimize.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action :\n",
    "            Action to probe the problem.\n",
    "        observation :\n",
    "            Observation of the problem for the given ``action``.\n",
    "        fun_params :\n",
    "            Belief over the parameters of the objective function.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Compute action via policy\n",
    "            action = self.policy(fun, self.fun_params)\n",
    "\n",
    "            # Make an observation\n",
    "            observation = self.observation_operator(fun, action)\n",
    "\n",
    "            # Belief update\n",
    "            self.fun_params = self.belief_update(self.fun_params, action, observation)\n",
    "\n",
    "            yield action, observation, self.fun_params\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        fun: Callable[[FloatArgType], FloatArgType],\n",
    "        callback: Optional[\n",
    "            Callable[[float, float, randvars.RandomVariable], None]\n",
    "        ] = None,\n",
    "    ) -> Tuple[float, randvars.RandomVariable, randvars.RandomVariable, Dict]:\n",
    "        \"\"\"Optimize the quadratic objective function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fun :\n",
    "            Quadratic objective function to optimize.\n",
    "        callback :\n",
    "            Callback function returning intermediate quantities of the\n",
    "            optimization loop. Note that depending on the function\n",
    "            supplied, this can slow down the solver considerably.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_opt :\n",
    "            Estimated minimum of ``fun``.\n",
    "        fun_opt :\n",
    "            Belief over the optimal value of the objective function.\n",
    "        fun_params :\n",
    "            Belief over the parameters of the objective function.\n",
    "        info :\n",
    "            Additional information about the optimization, e.g. convergence.\n",
    "        \"\"\"\n",
    "        # Setup\n",
    "        _has_converged = False\n",
    "        iteration = 0\n",
    "        optimization_iterator = self.optim_iterator(fun=fun)\n",
    "\n",
    "        # Evaluate stopping criteria\n",
    "        _has_converged, conv_crit = self.has_converged(fun=fun, iteration=iteration)\n",
    "\n",
    "        while not _has_converged:\n",
    "\n",
    "            # Perform one iteration of the optimizer\n",
    "            action, observation, _ = next(optimization_iterator)\n",
    "\n",
    "            # Callback function\n",
    "            if callback is not None:\n",
    "                callback(action, observation, self.fun_params)\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            # Evaluate stopping criteria\n",
    "            _has_converged, conv_crit = self.has_converged(fun=fun, iteration=iteration)\n",
    "\n",
    "        # Belief over optimal function value and optimum\n",
    "        x_opt, fun_opt = self.belief_optimum()\n",
    "\n",
    "        # Information (e.g. on convergence)\n",
    "        info = {\"iter\": iteration, \"conv_crit\": conv_crit}\n",
    "\n",
    "        return x_opt, fun_opt, self.fun_params, info\n",
    "\n",
    "    def belief_optimum(self) -> Tuple[float, randvars.RandomVariable]:\n",
    "        \"\"\"Compute the belief over the optimum and optimal function value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_opt :\n",
    "            Estimated minimum of ``fun``.\n",
    "        fun_opt :\n",
    "            Belief over the optimal value of the objective function.\n",
    "        \"\"\"\n",
    "        x_opt = -self.fun_params.mean[1] / self.fun_params.mean[0]\n",
    "        fun_opt = np.array([0.5 * x_opt ** 2, x_opt, 1]).T @ self.fun_params\n",
    "        return x_opt, fun_opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: Type Hints\n",
    "\n",
    "In order to decrease the number of bugs and increase maintainability these implementations must have [type\n",
    "hints](https://docs.python.org/3/library/typing.html). Type hints are as their name suggests a way to indicate the type of an object. Python is a dynamically typed language, meaning variable types are inferred at runtime not at compile time. Type hints allow static type checking instead, i.e. one can use automated tools to check whether function arguments of the wrong type are supplied without actually running the code.\n",
    "\n",
    "Additionally to the types provided by the `typing` module, ProbNum defines two kinds of custom types. These are defined to simplify and reuse commonly used types across modules.\n",
    "\n",
    "**API Types** are aliases which define custom types used throughout the library. Objects of\n",
    "this type may be supplied as arguments or returned by a method. For example if we pass a shape to a function, then we would use a `ShapeType`, defined as\n",
    "\n",
    "```python\n",
    "ShapeType = Tuple[int, ...]\n",
    "```\n",
    "where the ellipsis (`...`) denotes arbitrary length of the tuple of `int`s.\n",
    "\n",
    "**Argument Types** are aliases which define commonly used method arguments. These should\n",
    "only ever be used in the signature of a method and then be converted internally, e.g.\n",
    "in a class instantiation or an interface. They enable the user to conveniently\n",
    "specify a variety of object types for the same argument, while ensuring a unified\n",
    "internal representation of those same objects. Canonical examples are different kinds of integer or float types, which might be passed by a user. These are all unified internally.\n",
    "\n",
    "```python\n",
    "IntArgType = Union[int, numbers.Integral, np.integer]\n",
    "FloatArgType = Union[float, numbers.Real, np.floating]\n",
    "\n",
    "ShapeArgType = Union[IntArgType, Iterable[IntArgType]]\n",
    "\"\"\"Type of a public API argument for supplying a shape. Values of this type should\n",
    "always be converted into :class:`ShapeType` using the function\n",
    ":func:`probnum.utils.as_shape` before further internal processing.\"\"\"\n",
    "```\n",
    "\n",
    "As a small example we write a function which takes a shape and extends that shape with an integer. The type hinted implementation of this function would look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probnum.typing import ShapeType, IntArgType, ShapeArgType\n",
    "from probnum.utils import as_shape\n",
    "\n",
    "\n",
    "def extend_shape(shape: ShapeArgType, extension: IntArgType) -> ShapeType:\n",
    "    return as_shape(shape) + as_shape(extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now pass a shape which has a different type than `ShapeType`, we still get a `ShapeType` object back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of m: <class 'numpy.int32'>, n: <class 'int'> and l: <class 'numpy.int16'>.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "m = np.int32(2)\n",
    "n = 4\n",
    "l = np.int16(1)\n",
    "print(f\"Type of m: {type(m)}, n: {type(n)} and l: {type(l)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of newshape: <class 'tuple'>(<class 'int'>, <class 'int'>, <class 'int'>).\n"
     ]
    }
   ],
   "source": [
    "newshape = extend_shape([m, n], extension=l)\n",
    "print(f\"Type of newshape: {type(newshape)}({type(newshape[0])}, {type(newshape[1])}, {type(newshape[2])}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy, Action and Observation\n",
    "\n",
    "We now take a closer look at the components making up our PN method. Consider the exploration vs. exploitation policy explained in the [original tutorial](../tutorials/pn_methods.ipynb#Policy). Any policy is called with the objective function and the current belief over the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s explore_exploit_policy quadopt_example/policies\n",
    "def explore_exploit_policy(\n",
    "    fun: Callable[[FloatArgType], FloatArgType],\n",
    "    fun_params0: randvars.RandomVariable,\n",
    "    rng: np.random.Generator,\n",
    ") -> float:\n",
    "    \"\"\"Policy exploring around the estimate of the minimum based on the certainty about\n",
    "    the parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fun :\n",
    "        One-dimensional objective function.\n",
    "    fun_params0 :\n",
    "        Belief over the parameters of the quadratic objective.\n",
    "    rng :\n",
    "        Random number generator.\n",
    "    \"\"\"\n",
    "    a0, b0, _ = fun_params0\n",
    "    sample = randvars.Normal(0, np.trace(fun_params0.cov)).sample(rng=rng)\n",
    "    return -b0.mean / a0.mean + sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case since the policy is stochastic it has a `random_state` argument. The random number generator has to be set prior to initialization of `ProbabilisticQuadraticOptimizer`, since every policy is expected to have `QuadOptPolicyType`.\n",
    "\n",
    "```python\n",
    "QuadOptPolicyType = Callable[\n",
    "    [\n",
    "        Callable[[FloatArgType], FloatArgType],\n",
    "        randvars.RandomVariable\n",
    "    ],\n",
    "    FloatArgType,\n",
    "]\n",
    "```\n",
    "The observation process for this problem is very simple. It just evaluates the objective function. \n",
    "```python\n",
    "QuadOptObservationOperatorType = Callable[\n",
    "    [Callable[[FloatArgType], FloatArgType], FloatArgType], FloatArgType\n",
    "]\n",
    "```\n",
    "One can imagine a different probabilistic optimization method which evaluates the gradient as well. In this case the different observation processes would all get the function, its gradient and an evaluation point / action as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s function_evaluation quadopt_example/observation_operators\n",
    "def function_evaluation(\n",
    "    fun: Callable[[FloatArgType], FloatArgType], action: FloatArgType\n",
    ") -> np.float_:\n",
    "    \"\"\"Observe a (noisy) function evaluation of the quadratic objective.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fun :\n",
    "        Quadratic objective function to optimize.\n",
    "    action :\n",
    "        Input to the objective function.\n",
    "    \"\"\"\n",
    "    observation = fun(action)\n",
    "    try:\n",
    "        return utils.as_numpy_scalar(observation, dtype=np.floating)\n",
    "    except TypeError as exc:\n",
    "        raise TypeError(\n",
    "            \"The given argument `p` can not be cast to a `np.floating` object.\"\n",
    "        ) from exc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belief Update\n",
    "\n",
    "The belief update follows the same design pattern as the policy and observation process. Any belief update is passed the current belief over the parameters, an action and an observation.\n",
    "```python\n",
    "QuadOptBeliefUpdateType = Callable[\n",
    "    [\n",
    "        randvars.RandomVariable,\n",
    "        FloatArgType,\n",
    "        FloatArgType,\n",
    "    ],\n",
    "    randvars.RandomVariable,\n",
    "]\n",
    "```\n",
    "In this simple example we assume the noise covariance to be known and it is therefore not part of the call to the belief update but a separate argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s gaussian_belief_update quadopt_example/belief_updates\n",
    "def gaussian_belief_update(\n",
    "    fun_params0: randvars.RandomVariable,\n",
    "    action: FloatArgType,\n",
    "    observation: FloatArgType,\n",
    "    noise_cov: Union[np.ndarray, linops.LinearOperator],\n",
    ") -> randvars.RandomVariable:\n",
    "    \"\"\"Update the belief over the parameters with an observation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fun_params0 :\n",
    "        Belief over the parameters of the quadratic objective.\n",
    "    action :\n",
    "        Action of the probabilistic quadratic optimizer.\n",
    "    observation :\n",
    "        Observation of the problem corresponding to the given `action`.\n",
    "    noise_cov :\n",
    "        *shape=(3, 3)* -- Covariance of the noise on the parameters of the quadratic\n",
    "        objective given by the assumed observation model.\n",
    "    \"\"\"\n",
    "    # Feature vector\n",
    "    x = np.asarray(action).reshape(1, -1)\n",
    "    Phi = np.vstack((0.5 * x ** 2, x, np.ones_like(x)))\n",
    "\n",
    "    # Mean and covariance\n",
    "    mu = fun_params0.mean\n",
    "    Sigma = fun_params0.cov\n",
    "\n",
    "    # Gram matrix\n",
    "    gram = Phi.T @ (Sigma + noise_cov) @ Phi\n",
    "\n",
    "    # Posterior Mean\n",
    "    m = mu + Sigma @ Phi @ np.linalg.solve(gram, observation - Phi.T @ mu)\n",
    "\n",
    "    # Posterior Covariance\n",
    "    S = Sigma - Sigma @ Phi @ np.linalg.solve(gram, Phi.T @ Sigma)\n",
    "\n",
    "    return randvars.Normal(mean=m, cov=S)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Criteria\n",
    "\n",
    "The stopping criteria are also implemented as simple methods, which return a `bool` determining convergence and a string giving the name of the criterion.\n",
    "```python\n",
    "QuadOptStoppingCriterionType = Callable[\n",
    "    [Callable[[FloatArgType], FloatArgType], randvars.RandomVariable, IntArgType],\n",
    "    Tuple[bool, Union[str, None]],\n",
    "]\n",
    "```\n",
    "As an example consider the parameter uncertainty based stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s parameter_uncertainty quadopt_example/stopping_criteria\n",
    "def parameter_uncertainty(\n",
    "    fun: Callable[[FloatArgType], FloatArgType],\n",
    "    fun_params0: randvars.RandomVariable,\n",
    "    current_iter: IntArgType,\n",
    "    abstol: FloatArgType,\n",
    "    reltol: FloatArgType,\n",
    ") -> Tuple[bool, Union[str, None]]:\n",
    "    \"\"\"Termination based on numerical uncertainty about the parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fun :\n",
    "        One-dimensional objective function.\n",
    "    fun_params0 :\n",
    "        Belief over the parameters of the quadratic objective.\n",
    "    current_iter :\n",
    "        Current iteration of the PN method.\n",
    "    abstol :\n",
    "        Absolute convergence tolerance.\n",
    "    reltol :\n",
    "        Relative convergence tolerance.\n",
    "    \"\"\"\n",
    "    # Uncertainty over parameters given by the trace of the covariance.\n",
    "    trace_cov = np.trace(fun_params0.cov)\n",
    "    if trace_cov < abstol:\n",
    "        return True, \"uncertainty_abstol\"\n",
    "    elif trace_cov < np.linalg.norm(fun_params0.mean, ord=2) ** 2 * reltol:\n",
    "        return True, \"uncertainty_reltol\"\n",
    "    else:\n",
    "        return False, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see, one can pass multiple stopping criteria to the solver, which will all be checked in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing a Custom `ProbabilisticQuadraticOptimizer`\n",
    "\n",
    "Finally, we will demonstrate how to use the compositional pattern we implemented to create a custom `ProbabilisticQuadraticOptimizer`. In this fashion one can easily extend the interface `probsolve_qp` with new variants as a developer or flexibly adjust the internal parts of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from quadopt_example import ProbabilisticQuadraticOptimizer\n",
    "from quadopt_example.policies import stochastic_policy\n",
    "from quadopt_example.observation_operators import function_evaluation\n",
    "from quadopt_example.belief_updates import gaussian_belief_update\n",
    "from quadopt_example.stopping_criteria import maximum_iterations\n",
    "\n",
    "# Custom stopping criterion based on residual\n",
    "def residual(\n",
    "    fun,\n",
    "    fun_params0,\n",
    "    current_iter,\n",
    "    abstol=10 ** -5,\n",
    "    reltol=10 ** -5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Termination based on the residual.\n",
    "    \"\"\"\n",
    "    a, b, c = fun_params0.mean\n",
    "    resid = np.abs(fun(1.0) - (0.5 * a + b + c))\n",
    "    if resid < abstol:\n",
    "        return True, \"residual_abstol\"\n",
    "    elif resid < np.abs(fun(1.0)) * reltol:\n",
    "        return True, \"residual_reltol\"\n",
    "    else:\n",
    "        return False, None\n",
    "\n",
    "policy = partial(stochastic_policy, rng=rng)\n",
    "    \n",
    "# Compose custom PN method\n",
    "quadopt = ProbabilisticQuadraticOptimizer(\n",
    "    fun_params_prior=randvars.Normal(np.zeros(3), np.eye(3)),\n",
    "    policy=policy,\n",
    "    observation_operator=function_evaluation,\n",
    "    belief_update=partial(gaussian_belief_update, noise_cov=np.zeros(3)),\n",
    "    stopping_criteria=[residual, partial(maximum_iterations, maxiter=10)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0.16438006967466792\n",
      "Observation: 0.12253755887276303\n",
      "Parameter estimate:[0.00161169 0.01960929 0.11929241]\n",
      "\n",
      "Action: 0.8598811846127368\n",
      "Observation: 0.9908863556118175\n",
      "Parameter estimate:[ 0.5091651   0.98776347 -0.04671009]\n",
      "\n",
      "Action: 1.761661236511811\n",
      "Observation: 4.997571635247197\n",
      "Parameter estimate:[ 4.  -0.8  0.2]\n",
      "\n",
      "Algorithm converged with stopping criterion `residual_abstol`.\n"
     ]
    }
   ],
   "source": [
    "# Step through solver iteration\n",
    "i = 0\n",
    "for action, observation, fun_params in quadopt.optim_iterator(fun=fun):\n",
    "\n",
    "    print(f\"Action: {action}\\nObservation: {observation}\\nParameter estimate:{fun_params.mean}\\n\")\n",
    "    _has_converged, conv_crit = quadopt.has_converged(fun=fun, iteration=i)\n",
    "\n",
    "    if _has_converged:\n",
    "        print(f\"Algorithm converged with stopping criterion `{conv_crit}`.\")\n",
    "        break\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our new stopping criterion is hit after three iterations on an exact problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests and Examples\n",
    "\n",
    "While or even before you add a new PN method, write tests for its functionality. Writing tests before the \n",
    "code forces you to think about what your numerical method should do independent of its implementation. Some help for writing tests for PN methods can be found in the [\"Writing Tests\" developer guide](../development/unit_testing.ipynb).\n",
    "\n",
    "Once you are done, consider writing an [example notebook](../development/creating_an_example_notebook.ipynb)\n",
    "explaining how to use your new method and its benefits. This will encourage others to try it and gives you a chance to show off your hard work.\n",
    "\n",
    "Congratulations you just implemented your first probabilistic numerical method in ProbNum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
